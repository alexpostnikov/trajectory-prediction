{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, bidirectional = True):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, bidirectional = bidirectional)\n",
    "  \n",
    "    def forward(self, inputs, hidden):\n",
    "        output, hidden = self.lstm(inputs, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self,bs=1,num_l=1):\n",
    "        return (torch.zeros(num_l*(1 + int(self.bidirectional)), bs, self.hidden_size),\n",
    "          torch.zeros(num_l*(1 + int(self.bidirectional)), bs, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder(nn.Module):\n",
    "  \n",
    "    def __init__(self, hidden_size, output_size, vocab_size, input_size=30):\n",
    "        super(AttentionDecoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.attn = nn.Linear(hidden_size + input_size, 1)\n",
    "        self.lstm = nn.LSTM(hidden_size + vocab_size, output_size) #if we are using embedding hidden_size should be added with embedding of vocab size\n",
    "        self.final = nn.Linear(output_size, vocab_size)\n",
    "\n",
    "    def init_hidden(self,bs=1,num_l=1):\n",
    "        return (torch.zeros(num_l, bs, self.hidden_size),\n",
    "          torch.zeros(num_l, bs, self.hidden_size))\n",
    "  \n",
    "    \n",
    "    def forward(self, decoder_hidden, encoder_outputs, input):\n",
    "    \n",
    "        weights = []\n",
    "        print (decoder_hidden[0].shape)\n",
    "        print (\"encoder_outputs \", encoder_outputs.shape)\n",
    "        print (input.shape)\n",
    "        weights = self.attn(torch.cat((decoder_hidden[0].repeat(input.shape[0],1,1), input), dim = 2))\n",
    "        print (weights.shape)\n",
    "\n",
    "        normalized_weights = F.softmax(weights.squeeze()).unsqueeze(0)\n",
    "        \n",
    "        print (\"normalized_weights \", normalized_weights.shape)\n",
    "        print (encoder_outputs.view(1, -1, self.hidden_size).shape)\n",
    "        \n",
    "        attn_applied = torch.bmm(normalized_weights.unsqueeze(1),\n",
    "                                 encoder_outputs)\n",
    "        print (\"attn_applied \", attn_applied.shape)\n",
    "        print(\"input[0].shape\", input[0].shape)\n",
    "        input_lstm = torch.cat((attn_applied[0], input[0]), dim = 1) #if we are using embedding, use embedding of input here instead\n",
    "        print (input_lstm.shape)\n",
    "        output, hidden = self.lstm(input_lstm.unsqueeze(0), decoder_hidden)\n",
    "\n",
    "        output = self.final(output[0])\n",
    "\n",
    "        return output, hidden, normalized_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.shape  torch.Size([2, 1, 40])\n",
      "b[0].shape  torch.Size([2, 1, 20])\n",
      "b[1].shape torch.Size([2, 1, 20])\n",
      "torch.Size([1, 1, 40])\n",
      "encoder_outputs  torch.Size([4, 1, 40])\n",
      "torch.Size([2, 1, 30])\n",
      "torch.Size([2, 1, 1])\n",
      "normalized_weights  torch.Size([1, 2])\n",
      "torch.Size([1, 4, 40])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-219-67bef5f6f0d7>:26: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  normalized_weights = F.softmax(weights.squeeze()).unsqueeze(0)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor to have size 1 at dimension 0, but got size 4 for argument #2 'batch2' (while checking arguments for bmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-220-8d5132810d1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttentionDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbidirectional\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Assuming <SOS> to be all zeros\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y.shape \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-219-67bef5f6f0d7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, decoder_hidden, encoder_outputs, input)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mencoder_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         attn_applied = torch.bmm(normalized_weights.unsqueeze(1),\n\u001b[0m\u001b[1;32m     32\u001b[0m                                  encoder_outputs)\n\u001b[1;32m     33\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"attn_applied \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_applied\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor to have size 1 at dimension 0, but got size 4 for argument #2 'batch2' (while checking arguments for bmm)"
     ]
    }
   ],
   "source": [
    "bidirectional = True\n",
    "c = Encoder(input_size=10, hidden_size=20, bidirectional=bidirectional)\n",
    "enc_hidden = c.init_hidden(num_l=1)\n",
    "a, b = c.forward(torch.randn(2,1,10), enc_hidden)\n",
    "print(\"a.shape \", a.shape)\n",
    "print(\"b[0].shape \", b[0].shape)\n",
    "print(\"b[1].shape\", b[1].shape)\n",
    "\n",
    "x = AttentionDecoder(hidden_size=20 * (1 + bidirectional), output_size=25, vocab_size=30)\n",
    "y, z, w = x.forward(x.init_hidden(), torch.cat((a,a)), torch.zeros(2, 1, 30)) #Assuming <SOS> to be all zeros\n",
    "print(\"y.shape \", y.shape)\n",
    "\n",
    "print(\"z[0].shape \", z[0].shape)\n",
    "print(\"z[1].shape \", z[1].shape)\n",
    "print(\"w \", w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [1 x 1010], m2: [20 x 20] at /opt/conda/conda-bld/pytorch_1587428094786/work/aten/src/TH/generic/THTensorMath.cpp:41",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-246-8e52b9bf7af4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# input_length = input_tensor.size(0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# target_length = target_tensor.size(0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/pytorch_env/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-80e605fe51e5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         attn_weights = F.softmax(\n\u001b[0;32m---> 21\u001b[0;31m             self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n\u001b[0m\u001b[1;32m     22\u001b[0m         attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n\u001b[1;32m     23\u001b[0m                                  encoder_outputs.unsqueeze(0))\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch_env/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch_env/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch_env/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1608\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1609\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1610\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1611\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1612\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [1 x 1010], m2: [20 x 20] at /opt/conda/conda-bld/pytorch_1587428094786/work/aten/src/TH/generic/THTensorMath.cpp:41"
     ]
    }
   ],
   "source": [
    "model = AttnDecoderRNN(10,10)\n",
    "test = torch.rand(10, 10).type(torch.LongTensor)\n",
    "hidden = torch.zeros(1, 1, 10)\n",
    "encoder_outputs = torch.zeros(10, 10)\n",
    "# input_length = input_tensor.size(0)\n",
    "# target_length = target_tensor.size(0)\n",
    "model(test, hidden, encoder_outputs)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.rand(2,3,2,2)\n",
    "nor = torch.norm(t,dim=-1)\n",
    "sort = torch.sort(nor, dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.7027, 0.2775],\n",
      "          [0.8796, 0.2359]],\n",
      "\n",
      "         [[0.8802, 0.0744],\n",
      "          [0.6797, 0.5256]],\n",
      "\n",
      "         [[0.9282, 0.2064],\n",
      "          [0.5283, 0.5215]]],\n",
      "\n",
      "\n",
      "        [[[0.5255, 0.5019],\n",
      "          [0.1919, 0.9171]],\n",
      "\n",
      "         [[0.7988, 0.0710],\n",
      "          [0.3204, 0.0604]],\n",
      "\n",
      "         [[0.9466, 0.7941],\n",
      "          [0.5817, 0.6739]]]]) torch.Size([2, 3, 2, 2])\n",
      "tensor([[[0.7555, 0.9107],\n",
      "         [0.8833, 0.8592],\n",
      "         [0.9509, 0.7423]],\n",
      "\n",
      "        [[0.7267, 0.9370],\n",
      "         [0.8020, 0.3260],\n",
      "         [1.2356, 0.8902]]]) torch.Size([2, 3, 2])\n",
      "sort[1]  tensor([[[0, 1],\n",
      "         [1, 0],\n",
      "         [1, 0]],\n",
      "\n",
      "        [[0, 1],\n",
      "         [1, 0],\n",
      "         [1, 0]]]) torch.Size([2, 3, 2])\n",
      "sort[0]  tensor([[[0.7555, 0.9107],\n",
      "         [0.8592, 0.8833],\n",
      "         [0.7423, 0.9509]],\n",
      "\n",
      "        [[0.7267, 0.9370],\n",
      "         [0.3260, 0.8020],\n",
      "         [0.8902, 1.2356]]]) torch.Size([2, 3, 2])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[3, 2]' is invalid for input of size 24",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-310-4c6106448e5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# torch.cat( [nor[i][sort[1][i]] for i in range(2)]).reshape(nor.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0msort\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m#(sort[1][0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-310-4c6106448e5b>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# torch.cat( [nor[i][sort[1][i]] for i in range(2)]).reshape(nor.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0msort\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m#(sort[1][0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[3, 2]' is invalid for input of size 24"
     ]
    }
   ],
   "source": [
    "print (t, t.shape)\n",
    "print (nor, nor.shape)\n",
    "print (\"sort[1] \", sort[1], sort[1].shape)\n",
    "print (\"sort[0] \", sort[0], sort[0].shape)\n",
    "\n",
    "# torch.cat( [nor[i][sort[1][i]] for i in range(2)]).reshape(nor.shape)\n",
    "sort = torch.cat([torch.cat([t[i][j] for j in sort[1][i]]).reshape(3,2) for i in [0, 1]]).reshape(t.shape)   #(sort[1][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = torch.rand(2,3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9648, 0.8062, 0.3164],\n",
      "        [0.4375, 0.7637, 0.5865]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.3164, 0.4375, 0.5865],\n",
       "        [0.7637, 0.8062, 0.9648]])"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt_n = torch.norm(tt,dim=-1)\n",
    "tt_f = tt_n.flatten()\n",
    "\n",
    "sort = torch.sort(tt_f, dim=-1)\n",
    "print (tt_n)\n",
    "tt_f[sort[1]].reshape(tt_n.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "ra = torch.rand(2,2,10)\n",
    "r = torch.zeros(2,2,20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7028, 0.8551],\n",
      "        [0.8610, 0.0162]])\n",
      "tensor([[1, 0],\n",
      "        [0, 1]])\n",
      "tensor([[0.8610, 0.0162],\n",
      "        [0.7028, 0.8551]])\n",
      "tensor([[0.7028, 0.8610],\n",
      "        [0.8551, 0.0162]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.rand(2,2)\n",
    "print(t)\n",
    "mask = torch.eye(2).long()\n",
    "print (mask)\n",
    "print(t[torch.tensor([1,0])])\n",
    "print (t.permute(1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AdditiveAttention(nn.Module):\n",
    "    # Implementing the attention module of Bahdanau et al. 2015 where\n",
    "    # score(h_j, s_(i-1)) = v . tanh(W_1 h_j + W_2 s_(i-1))\n",
    "    def __init__(self, encoder_hidden_state_dim, decoder_hidden_state_dim, internal_dim=None):\n",
    "        super(AdditiveAttention, self).__init__()\n",
    "\n",
    "        if internal_dim is None:\n",
    "            internal_dim = int((encoder_hidden_state_dim + decoder_hidden_state_dim) / 2)\n",
    "\n",
    "        self.w1 = nn.Linear(encoder_hidden_state_dim, internal_dim, bias=False)\n",
    "        self.w2 = nn.Linear(decoder_hidden_state_dim, internal_dim, bias=False)\n",
    "        self.v = nn.Linear(internal_dim, 1, bias=False)\n",
    "\n",
    "    def score(self, encoder_state, decoder_state):\n",
    "        # encoder_state is of shape (batch, enc_dim)\n",
    "        # decoder_state is of shape (batch, dec_dim)\n",
    "        # return value should be of shape (batch, 1)\n",
    "        return self.v(torch.tanh(self.w1(encoder_state) + self.w2(decoder_state)))\n",
    "\n",
    "    def forward(self, encoder_states, decoder_state):\n",
    "        # encoder_states is of shape (batch, num_enc_states, enc_dim)\n",
    "        # decoder_state is of shape (batch, dec_dim)\n",
    "        score_vec = torch.cat([self.score(encoder_states[:, i], decoder_state) for i in range(encoder_states.shape[1])],\n",
    "                              dim=1)\n",
    "        # score_vec is of shape (batch, num_enc_states)\n",
    "\n",
    "        attention_probs = torch.unsqueeze(F.softmax(score_vec, dim=1), dim=2)\n",
    "        # attention_probs is of shape (batch, num_enc_states, 1)\n",
    "\n",
    "        final_context_vec = torch.sum(attention_probs * encoder_states, dim=1)\n",
    "        # final_context_vec is of shape (batch, enc_dim)\n",
    "\n",
    "        return final_context_vec, attention_probs\n",
    "\n",
    "\n",
    "class TemporallyBatchedAdditiveAttention(AdditiveAttention):\n",
    "    # Implementing the attention module of Bahdanau et al. 2015 where\n",
    "    # score(h_j, s_(i-1)) = v . tanh(W_1 h_j + W_2 s_(i-1))\n",
    "    def __init__(self, encoder_hidden_state_dim, decoder_hidden_state_dim, internal_dim=None):\n",
    "        super(TemporallyBatchedAdditiveAttention, self).__init__(encoder_hidden_state_dim,\n",
    "                                                                 decoder_hidden_state_dim,\n",
    "                                                                 internal_dim)\n",
    "\n",
    "    def score(self, encoder_state, decoder_state):\n",
    "        # encoder_state is of shape (batch, num_enc_states, max_time, enc_dim)\n",
    "        # decoder_state is of shape (batch, max_time, dec_dim)\n",
    "        # return value should be of shape (batch, num_enc_states, max_time, 1)\n",
    "        return self.v(torch.tanh(self.w1(encoder_state) + torch.unsqueeze(self.w2(decoder_state), dim=1)))\n",
    "\n",
    "    def forward(self, encoder_states, decoder_state):\n",
    "        # encoder_states is of shape (batch, num_enc_states, max_time, enc_dim)\n",
    "        # decoder_state is of shape (batch, max_time, dec_dim)\n",
    "        score_vec = self.score(encoder_states, decoder_state)\n",
    "        # score_vec is of shape (batch, num_enc_states, max_time, 1)\n",
    "\n",
    "        attention_probs = F.softmax(score_vec, dim=1)\n",
    "        # attention_probs is of shape (batch, num_enc_states, max_time, 1)\n",
    "\n",
    "        final_context_vec = torch.sum(attention_probs * encoder_states, dim=1)\n",
    "        # final_context_vec is of shape (batch, max_time, enc_dim)\n",
    "\n",
    "        return final_context_vec, torch.squeeze(torch.transpose(attention_probs, 1, 2), dim=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 8, 5])"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att = TemporallyBatchedAdditiveAttention(encoder_hidden_state_dim=5, decoder_hidden_state_dim=20, internal_dim=10)\n",
    "inp = torch.rand(3,8,2)\n",
    "encoder_states = torch.rand(3,8,5)\n",
    "decoder_state = torch.rand(3,8,20)\n",
    "att(encoder_states, decoder_state)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
