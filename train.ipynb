{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from dataloader import Dataset_from_pkl, is_filled\n",
    "from model import LSTMTagger, LSTM_hid, LSTM_simple, OneLayer, LSTM_single, LSTM_single_with_emb\n",
    "torch.manual_seed(1)\n",
    "from tqdm import tqdm\n",
    "from utils import compare_prediction_gt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training on poses\n",
    "1.37 lstm_tag \n",
    "<br>\n",
    "1.19 lstm_hid\n",
    "<br>\n",
    "2.01   lstm_simple\n",
    "<br>\n",
    "2.1   OneLayer\n",
    "<br>\n",
    "1.8 lstm_single \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_observation_other_gt_poses(local_batch, poses=True):\n",
    "    \"\"\"\n",
    "    for each agent in scene generate obseravtion, other_pose, future_poses\n",
    "    observations -> list[tensors of shape [1,1,20,2]]]\n",
    "    others -> list[tensors of shape [1,num_others,20,2]]]\n",
    "    future_poses -> list[tensors of shape [1,1,12,2]]]\n",
    "    \"\"\"\n",
    "\n",
    "    obseravtions = []\n",
    "    gts = []\n",
    "    others = []\n",
    "    \n",
    "    for ped in range(local_batch.shape[1]):\n",
    "        observed_pose = local_batch[:, ped, :,2:4]\n",
    "        obseravtions.append(observed_pose)\n",
    "        gt_future = local_batch[:, ped:ped+1, 8:, 2:4]\n",
    "        gts.append(gt_future)\n",
    "        others_poses = torch.cat((local_batch[:, 0:ped, :, 2:4], local_batch[:, ped+1:, :,2:4]),dim=1)\n",
    "        others.append(others_poses)\n",
    "\n",
    "    return obseravtions, gts, others\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_traj(data, ax=None,color=\"red\"):\n",
    "    \n",
    "    #data shape [n, 2]\n",
    "    x_poses = np.zeros((data.shape[0], data.shape[1]))\n",
    "    y_poses = np.zeros((data.shape[0], data.shape[1]))\n",
    "    for person in range(data.shape[0]):\n",
    "        x_poses[person]  = data[person, :, 0].numpy()\n",
    "        y_poses[person]  = data[person, :, 1].numpy()\n",
    "    \n",
    "    for person in range(data.shape[0]):\n",
    "        if ax is not None:\n",
    "            ax.plot(x_poses[person], y_poses[person], 'o-', color=color)\n",
    "        else:\n",
    "            plt.plot(x_poses[person], y_poses[person], 'o-', color=color)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading eth_train.pkl\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "training_set = Dataset_from_pkl(\"/home/robot/repos/trajectories_pred/processed/\", data_files=[\"eth_train.pkl\"])\n",
    "training_generator = torch.utils.data.DataLoader(training_set, batch_size=1)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print (device)\n",
    "model  = LSTM_hid(10, 2, 2).to(device)\n",
    "# model  = LSTM_simple(10, 2, 2).to(device)\n",
    "# model  = OneLayer().to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "drop_every_epochs = 5\n",
    "drop_rate = 0.4\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, drop_every_epochs, drop_rate) # every drop_every_epochs epochs drop by drop_rate lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training in separated manner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss 2.9254, time taken 21.60\n",
      "epoch 1 loss 2.1336, time taken 21.45\n",
      "epoch 2 loss 2.0145, time taken 21.83\n",
      "epoch 3 loss 1.9000, time taken 21.44\n",
      "epoch 4 loss 1.8290, time taken 21.44\n",
      "epoch 5 loss 1.8950, time taken 21.49\n",
      "epoch 6 loss 1.7283, time taken 21.51\n",
      "epoch 7 loss 1.6438, time taken 21.30\n",
      "epoch 8 loss 1.5493, time taken 21.49\n",
      "epoch 9 loss 1.5060, time taken 21.53\n",
      "epoch 10 loss 1.5299, time taken 21.46\n",
      "epoch 11 loss 1.4339, time taken 21.45\n",
      "epoch 12 loss 1.4058, time taken 21.36\n",
      "epoch 13 loss 1.3981, time taken 21.45\n",
      "epoch 14 loss 1.3729, time taken 21.58\n",
      "epoch 15 loss 1.3940, time taken 21.43\n",
      "epoch 16 loss 1.3475, time taken 21.54\n",
      "epoch 17 loss 1.3333, time taken 21.46\n",
      "epoch 18 loss 1.3257, time taken 21.47\n",
      "epoch 19 loss 1.3169, time taken 21.51\n",
      "epoch 20 loss 1.3144, time taken 21.50\n",
      "epoch 21 loss 1.3063, time taken 21.40\n",
      "epoch 22 loss 1.2994, time taken 21.48\n",
      "epoch 23 loss 1.2954, time taken 21.55\n",
      "epoch 24 loss 1.2922, time taken 21.61\n",
      "epoch 25 loss 1.2895, time taken 21.61\n",
      "epoch 26 loss 1.2862, time taken 21.74\n",
      "epoch 27 loss 1.2834, time taken 21.68\n",
      "epoch 28 loss 1.2817, time taken 21.67\n",
      "epoch 29 loss 1.2803, time taken 21.43\n",
      "epoch 30 loss 1.2783, time taken 21.44\n",
      "epoch 31 loss 1.2771, time taken 21.42\n",
      "epoch 32 loss 1.2763, time taken 21.44\n",
      "epoch 33 loss 1.2757, time taken 21.71\n",
      "epoch 34 loss 1.2751, time taken 21.61\n",
      "epoch 35 loss 1.2741, time taken 21.46\n",
      "epoch 36 loss 1.2736, time taken 21.62\n",
      "epoch 37 loss 1.2734, time taken 21.46\n",
      "epoch 38 loss 1.2731, time taken 21.66\n",
      "epoch 39 loss 1.2728, time taken 21.70\n",
      "epoch 40 loss 1.2724, time taken 21.63\n",
      "epoch 41 loss 1.2723, time taken 21.60\n",
      "epoch 42 loss 1.2721, time taken 21.61\n",
      "epoch 43 loss 1.2720, time taken 21.67\n",
      "epoch 44 loss 1.2719, time taken 21.68\n",
      "epoch 45 loss 1.2717, time taken 21.73\n",
      "epoch 46 loss 1.2717, time taken 21.66\n",
      "epoch 47 loss 1.2716, time taken 21.61\n",
      "epoch 48 loss 1.2716, time taken 21.67\n",
      "epoch 49 loss 1.2716, time taken 21.70\n",
      "epoch 50 loss 1.2715, time taken 21.71\n",
      "epoch 51 loss 1.2715, time taken 21.64\n",
      "epoch 52 loss 1.2714, time taken 21.60\n",
      "epoch 53 loss 1.2714, time taken 21.66\n",
      "epoch 54 loss 1.2714, time taken 21.62\n",
      "epoch 55 loss 1.2714, time taken 21.60\n",
      "epoch 56 loss 1.2714, time taken 21.60\n",
      "epoch 57 loss 1.2714, time taken 21.68\n",
      "epoch 58 loss 1.2714, time taken 21.63\n",
      "epoch 59 loss 1.2714, time taken 21.63\n",
      "epoch 60 loss 1.2713, time taken 21.68\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for epoch in range(0,61):\n",
    "    epoch_loss = 0\n",
    "    start = time.time()\n",
    "    for batch_id, local_batch in enumerate(training_generator):\n",
    "        if batch_id>100:\n",
    "            break\n",
    "        obseravtions, gts, others = get_observation_other_gt_poses(local_batch.to(device))\n",
    "        batch_loss = 0\n",
    "        for observation, gt, other in zip(obseravtions, gts, others):\n",
    "            model.zero_grad()\n",
    "            fil = is_filled(observation)\n",
    "            if fil:\n",
    "                predictions = torch.zeros(1, 1, 0, 2).requires_grad_(True).to(device)\n",
    "                for timestamp in range(0, 12):\n",
    "                    \n",
    "                    prediction = model(observation[:, 0+timestamp:8+timestamp, :], other[:, :, 0+timestamp:8+timestamp, :])\n",
    "                    predictions = torch.cat((predictions, prediction.unsqueeze(0)), dim=2)\n",
    "\n",
    "                loss = torch.sum(torch.norm(predictions - gt))\n",
    "                batch_loss += loss.item()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        if len(obseravtions) != 0:\n",
    "#             print(\"\\tid: {id}, batch_loss: {batch_loss:.2f}\".format(batch_loss=batch_loss/len(obseravtions), id=id))\n",
    "            epoch_loss += batch_loss/len(obseravtions)\n",
    "        \n",
    "    print (\"epoch {epoch} loss {el:0.4f}, time taken {t:0.2f}\".format(epoch=epoch, el=epoch_loss/batch_id,t=time.time()-start) )\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 12, 2])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for batch_id, local_batch in enumerate(training_generator):\n",
    "    if batch_id != 180:\n",
    "        continue\n",
    "    obseravtions, gts, others = get_observation_other_gt_poses(local_batch.to(device))\n",
    "    \n",
    "    for observation, gt, other in zip(obseravtions, gts, others):\n",
    "        gt = observation.clone()\n",
    "        predictions = torch.zeros(1, 1, 0, 2).requires_grad_(True).to(device)\n",
    "        for timestamp in range(0, 12):\n",
    "            prediction = model(observation[:, 0+timestamp:8+timestamp, :].clone(), other[:, :, 0+timestamp:8+timestamp, :].clone())\n",
    "            observation[:,9+t:10+t,:] = prediction.clone()\n",
    "            predictions = torch.cat((predictions, prediction.unsqueeze(0)), dim=2)\n",
    "        break\n",
    "    break\n",
    "pass\n",
    "\n",
    "\n",
    "data = gt[:,:,:].detach().cpu()\n",
    "\n",
    "f, ax = plt.subplots(1,2)\n",
    "plot_traj(data,ax[0])\n",
    "\n",
    "data = gt[:,0:8,:].detach().cpu()\n",
    "plot_traj(data,ax[1])\n",
    "plot_traj(predictions[0].detach().cpu(),ax[1], color=\"b\")\n",
    "\n",
    "ax[0].axis('equal')\n",
    "ax[1].axis('equal')\n",
    "ax[0].set_ylim(-7, 7)\n",
    "ax[1].set_ylim(-7, 7)\n",
    "\n",
    "print (predictions.shape)\n",
    "\n",
    "# data = torch.cat((data, predictions[0].detach().cpu()), dim=1)\n",
    "# plot_traj(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel manner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  single - 1.83\n",
    "###  single with emb- 1.57 (LSTM_single_with_emb(40, 20, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from dataloader import Dataset_from_pkl, is_filled\n",
    "from model import LSTMTagger, LSTM_hid, LSTM_simple, OneLayer, LSTM_single, LSTM_single_with_emb, LSTM_delta\n",
    "torch.manual_seed(1)\n",
    "from tqdm import tqdm\n",
    "from utils import compare_prediction_gt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def setup_experiment(title, logdir=\"./tb\"):\n",
    "    experiment_name = \"{}@{}\".format(title, datetime.now().strftime(\"%d.%m.%Y-%H:%M:%S\"))\n",
    "    writer = SummaryWriter(log_dir=os.path.join(logdir, experiment_name))\n",
    "    best_model_path = f\"{title}.best.pth\"\n",
    "    return writer, experiment_name, best_model_path\n",
    "\n",
    "\n",
    "def get_ade_fde(generator, model, limit=1e10):\n",
    "    ade = []\n",
    "    fde = []\n",
    "    for batch_id, local_batch in enumerate(generator):\n",
    "\n",
    "        if batch_id > limit:\n",
    "            break\n",
    "        if local_batch.shape[1] < 1:\n",
    "            continue\n",
    "        gt = local_batch.clone()\n",
    "        local_batch = local_batch.to(device)\n",
    "        num_peds = local_batch.shape[1]\n",
    "        predictions = torch.zeros(num_peds, 0, 2).requires_grad_(True).to(device)\n",
    "\n",
    "        for t in range(0, 12):\n",
    "            prediction = model(local_batch[0, :, 0 + t: 8 + t, 2:4])\n",
    "\n",
    "            #                 prediction = prediction.unsqueeze(1)\n",
    "            predictions = torch.cat((predictions, prediction), dim=1)\n",
    "            local_batch[0, :, 8 + t:9 + t, 2:4] = prediction.detach()\n",
    "\n",
    "        metrics = compare_prediction_gt(predictions.detach().cpu(), gt.detach().cpu()[0][:, :, 2:4])\n",
    "\n",
    "        for local_ade in metrics[\"ade\"]:\n",
    "            ade.append(local_ade)\n",
    "        for local_fde in metrics[\"fde\"]:\n",
    "            fde.append(local_fde)\n",
    "    ade = sum(ade) / len(ade)\n",
    "    fde = sum(fde) / len(fde)\n",
    "    return (ade, fde)\n",
    "\n",
    "\n",
    "def get_batch_is_filled_mask(batch):\n",
    "    num_peds = batch.shape[1]\n",
    "    mask = torch.zeros(num_peds, 12, 2)\n",
    "    full_peds = 0\n",
    "    for ped in range(num_peds):\n",
    "        if is_filled(batch[0][ped]):\n",
    "            mask[ped] = torch.ones(12, 2)\n",
    "            full_peds+=1\n",
    "    return mask, max(full_peds,1)\n",
    "\n",
    "def get_is_filled_mask(batch):\n",
    "    num_peds = batch.shape[1]\n",
    "    mask = torch.zeros(batch.shape[0], num_peds, 12, 2)\n",
    "    for b in range(batch.shape[0]):\n",
    "        submask = get_batch_is_filled_mask(batch[b:b+1])\n",
    "        mask[b, ...] = submask[0]\n",
    "    return  mask\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading eth_train.pkl\n",
      "loading eth_test.pkl\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "training_set = Dataset_from_pkl(\"/home/robot/repos/trajectory-prediction/processed/\", data_files=[\"eth_train.pkl\"])\n",
    "training_generator = torch.utils.data.DataLoader(training_set, batch_size=1)\n",
    "\n",
    "test_set = Dataset_from_pkl(\"/home/robot/repos/trajectory-prediction/processed/\", data_files=[\"eth_test.pkl\"])\n",
    "test_generator = torch.utils.data.DataLoader(training_set, batch_size=1)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print (device)\n",
    "# model  = LSTM_single(20, 2, 2).to(device)\n",
    "# model  = LSTM_simple(10, 2, 2).to(device)\n",
    "\n",
    "# model = LSTM_single_with_emb(40, 20, 2).to(device)\n",
    "model = LSTM_delta(40, 20, 2).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "drop_every_epochs = 10\n",
    "drop_rate = 0.4\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, drop_every_epochs, drop_rate) # every drop_every_epochs epochs drop by drop_rate lr\n",
    "writer, _, _ = setup_experiment(\"LSTM_single_with_emb\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 4, 20, 8])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [400 x 8], m2: [2 x 20] at /opt/conda/conda-bld/pytorch_1587428094786/work/aten/src/THC/generic/THCTensorMathBlas.cu:283",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-75bed4aa53ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mlocal_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocal_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlocal_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m#     for batch in range (0, batch_size):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch_env/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/trajectory-prediction/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, scene)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscene\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscene\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minp_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscene\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;31m# scene shape = num_peds, timestamps, data_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mlstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_future_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# lstm_out shape num_peds, timestamps ,  2*hidden_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch_env/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch_env/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch_env/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1610\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1611\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1612\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1614\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [400 x 8], m2: [2 x 20] at /opt/conda/conda-bld/pytorch_1587428094786/work/aten/src/THC/generic/THCTensorMathBlas.cu:283"
     ]
    }
   ],
   "source": [
    "batch_size  = 5\n",
    "batch_id = 0\n",
    "dataset_len = len(training_set)\n",
    "\n",
    "while batch_id < dataset_len:\n",
    "    local_batch = training_set[batch_id, batch_size]\n",
    "    batch_id += batch_size\n",
    "    mask = get_is_filled_mask(local_batch)\n",
    "    local_batch = local_batch.to(device)\n",
    "    print (local_batch.shape)\n",
    "    model(local_batch)\n",
    "#     for batch in range (0, batch_size):\n",
    "        \n",
    "#         mask, _ = get_batch_is_filled_mask(local_batch[batch:batch+1])\n",
    "    break\n",
    "\n",
    "\n",
    "# for batch_id, local_batch in enumerate(training_set[batch_id, batch_size]):\n",
    "#     batch_id+=batch_size\n",
    "#     print (local_batch.shape)\n",
    "#     break\n",
    "    # for batch_id, local_batch in enumerate(training_generator):\n",
    "#     if batch_id != 120:\n",
    "#         continue\n",
    "#     mask, _ = get_batch_is_filled_mask(local_batch)\n",
    "#     print (mask[:,0,0])\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss 1.2896, time taken 3.40, delta 1.290\n",
      "epoch 1 loss 1.0313, time taken 3.34, delta -0.258\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ed4eaf5f701c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m#             print(local_batch.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch_env/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/trajectory-prediction/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, scene)\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mcurrent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscene\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0mcurrent_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_inp_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mdistr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shape num_peds, timestamps ,  2*hidden_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m         \u001b[0mcatted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m18\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mtag_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscene\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden2tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcatted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch_env/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch_env/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m    570\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    571\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prev_epoch_loss = 0\n",
    "for epoch in range(0,61):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    start = time.time()\n",
    "    epoch_loss = 0\n",
    "    num_skipped = 0\n",
    "    for batch_id, local_batch in enumerate(training_generator):\n",
    "        if (batch_id>100):\n",
    "            break\n",
    "        local_batch = local_batch.to(device)\n",
    "        model.zero_grad()\n",
    "        num_peds = local_batch.shape[1]\n",
    "        predictions = torch.zeros(num_peds, 0, 2).requires_grad_(True).to(device)\n",
    "        mask, full_peds = get_batch_is_filled_mask(local_batch)\n",
    "        mask = mask.to(device)\n",
    "        gt = local_batch.clone()\n",
    "        if torch.sum(mask) == 0.0:\n",
    "            num_skipped += 1\n",
    "            continue\n",
    "        for t in range(0, 12):\n",
    "            prediction = model(local_batch[0,:,0+t:8+t,2:4])\n",
    "            predictions = torch.cat((predictions, prediction), dim=1)\n",
    "\n",
    "        loss = torch.sum(torch.norm(mask*(predictions - gt[0,:,8:,2:4])))\n",
    "        epoch_loss += loss.item()/full_peds\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    epoch_loss = epoch_loss/(batch_id - num_skipped)\n",
    "    print (\"epoch {epoch} loss {el:0.4f}, time taken {t:0.2f}, delta {delta:0.3f}\".format(epoch=epoch, el=epoch_loss,t=time.time()-start, delta=-prev_epoch_loss+epoch_loss) )\n",
    "    if writer is not None:\n",
    "            writer.add_scalar(f\"loss_epoch\", epoch_loss, epoch)\n",
    "\n",
    "    prev_epoch_loss = epoch_loss\n",
    "    scheduler.step()\n",
    "    model.eval()\n",
    "    ade, fde = get_ade_fde(test_generator, model,50)\n",
    "    if writer is not None:\n",
    "            writer.add_scalar(f\"ade\", ade, epoch)\n",
    "            writer.add_scalar(f\"fde\", fde, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 4 and 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-0c0eb7e7626c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#             print (prediction[3,:])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mlocal_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#             print (local_batch[0,3,9+t,2:4])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 4 and 3"
     ]
    }
   ],
   "source": [
    "for batch_id, local_batch in enumerate(training_generator):\n",
    "    if batch_id != 400:\n",
    "        continue\n",
    "    gt = local_batch.clone()\n",
    "    local_batch = local_batch.to(device)\n",
    "    num_peds = local_batch.shape[1]\n",
    "    predictions = torch.zeros(num_peds, 0, 2).requires_grad_(True).to(device)\n",
    "    \n",
    "    for t in range(0,12):\n",
    "#             print (local_batch[0,3,7+t:10+t,2:4])\n",
    "            prediction = model(local_batch[0,:,0+t:8+t,2:4])\n",
    "#             print (prediction[3,:])\n",
    "            prediction = prediction.unsqueeze(1)\n",
    "            predictions = torch.cat((predictions, prediction), dim=1)\n",
    "            local_batch[0,:,9+t:10+t,2:4] = prediction.detach()\n",
    "#             print (local_batch[0,3,9+t,2:4])\n",
    "        \n",
    "    break\n",
    "pass\n",
    "\n",
    "ped_num = 1\n",
    "\n",
    "data = gt[0,ped_num:ped_num+1,:,2:4].cpu()\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(9, 3))\n",
    "plot_traj(data, ax=ax[1],color=\"blue\")\n",
    "\n",
    "data = gt[0,:,:,2:4].cpu()\n",
    "plot_traj(data, ax=ax[0],color=\"blue\")\n",
    "\n",
    "# data = torch.cat((gt[0,ped_num:ped_num+1,0:8,2:4], predictions[ped_num:ped_num+1,:,:].detach().cpu()),dim=1)\\\n",
    "\n",
    "plot_traj(gt[0,ped_num:ped_num+1,0:8,2:4], ax[2], color=\"blue\")\n",
    "plot_traj(predictions[ped_num:ped_num+1, :, :].detach().cpu(),ax[2])\n",
    "\n",
    "ax[0].axis('equal')\n",
    "ax[1].axis('equal')\n",
    "ax[2].axis('equal')\n",
    "\n",
    "ax[0].set_ylim(-10, 10)\n",
    "ax[0].set_xlim(-10, 10)\n",
    "\n",
    "ax[1].set_ylim(-10, 10)\n",
    "ax[1].set_xlim(-10, 10)\n",
    "\n",
    "\n",
    "# data = obseravtions[0][:,:,:].detach().cpu()\n",
    "# plot_traj(data)\n",
    "# data = obseravtions[0][:,0:8,:].detach().cpu()\n",
    "# data = torch.cat((data, predictions[0].detach().cpu()), dim=1)\n",
    "# plot_traj(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [6 x 8], m2: [2 x 20] at /opt/conda/conda-bld/pytorch_1587428094786/work/aten/src/THC/generic/THCTensorMathBlas.cu:283",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-a95d09715d04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ade_fde\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-aa49d65f0545>\u001b[0m in \u001b[0;36mget_ade_fde\u001b[0;34m(generator, model, limit)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m#                 prediction = prediction.unsqueeze(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch_env/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/trajectory-prediction/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, scene)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscene\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscene\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minp_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscene\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;31m# scene shape = num_peds, timestamps, data_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mlstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_future_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# lstm_out shape num_peds, timestamps ,  2*hidden_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch_env/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch_env/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch_env/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1610\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1611\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1612\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1614\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [6 x 8], m2: [2 x 20] at /opt/conda/conda-bld/pytorch_1587428094786/work/aten/src/THC/generic/THCTensorMathBlas.cu:283"
     ]
    }
   ],
   "source": [
    "get_ade_fde(training_generator, model, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "sum([1,2,3,4])/len([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.2000e+01,  1.8000e+02, -8.3340e-01, -2.1798e+00, -2.9465e-01,\n",
      "          1.3365e+00,  7.5636e-03,  5.2207e-03],\n",
      "        [ 3.2000e+01,  1.8100e+02, -9.5127e-01, -1.6452e+00, -2.8860e-01,\n",
      "          1.3407e+00,  8.1884e-02,  5.7800e-02],\n",
      "        [ 3.2000e+01,  1.8200e+02, -1.0643e+00, -1.1072e+00, -2.2914e-01,\n",
      "          1.3827e+00,  1.4141e-01,  9.9939e-02],\n",
      "        [ 3.2000e+01,  1.8300e+02, -1.1346e+00, -5.3900e-01, -1.7548e-01,\n",
      "          1.4206e+00, -2.0257e-01, -2.0957e-01],\n",
      "        [ 3.2000e+01,  1.8400e+02, -1.2047e+00,  2.9251e-02, -3.9120e-01,\n",
      "          1.2151e+00, -8.8790e-01, -4.7695e-01],\n",
      "        [ 3.2000e+01,  1.8500e+02, -1.4475e+00,  4.3306e-01, -8.8580e-01,\n",
      "          1.0391e+00, -1.0918e+00, -1.9279e-01],\n",
      "        [ 3.2000e+01,  1.8600e+02, -1.9133e+00,  8.6050e-01, -1.2646e+00,\n",
      "          1.0608e+00, -5.7516e-01, -8.5768e-03],\n",
      "        [ 3.2000e+01,  1.8700e+02, -2.4592e+00,  1.2817e+00, -1.3459e+00,\n",
      "          1.0322e+00, -8.2871e-02, -1.1336e-01],\n",
      "        [ 3.2000e+01,  1.8800e+02, -2.9900e+00,  1.6863e+00, -1.3309e+00,\n",
      "          9.7015e-01, -6.9059e-03, -3.3673e-01],\n",
      "        [ 3.2000e+01,  1.8900e+02, -3.5240e+00,  2.0579e+00, -1.3514e+00,\n",
      "          7.6282e-01, -3.9791e-02, -5.2431e-01],\n",
      "        [ 3.2000e+01,  1.9000e+02, -4.0712e+00,  2.2965e+00, -1.3628e+00,\n",
      "          5.5071e-01,  1.8745e-02, -5.5190e-01],\n",
      "        [ 3.2000e+01,  1.9100e+02, -4.6142e+00,  2.4984e+00, -1.3365e+00,\n",
      "          3.2130e-01,  5.9193e-02, -5.1610e-01],\n",
      "        [ 3.2000e+01,  1.9200e+02, -5.1404e+00,  2.5536e+00, -1.3154e+00,\n",
      "          1.3783e-01,  2.1474e-01, -3.3189e-01],\n",
      "        [ 3.2000e+01,  1.9300e+02, -5.6665e+00,  2.6087e+00, -1.1647e+00,\n",
      "          5.5787e-02,  5.0216e-01, -2.7371e-01],\n",
      "        [ 3.2000e+01,  1.9400e+02, -6.0721e+00,  2.5982e+00, -9.1368e-01,\n",
      "         -8.1144e-02,  4.3935e-01, -2.3941e-01],\n",
      "        [ 3.2000e+01,  1.9500e+02, -6.3975e+00,  2.5438e+00, -8.1318e-01,\n",
      "         -1.3574e-01,  1.2595e-01, -6.7869e-02],\n",
      "        [ 3.2000e+01,  1.9600e+02, -6.7226e+00,  2.4896e+00, -8.1292e-01,\n",
      "         -1.3544e-01,  6.5770e-04,  7.4581e-04],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "for batch_id, local_batch in enumerate(training_generator):\n",
    "    if batch_id != 180:\n",
    "        continue\n",
    "    print(local_batch[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': [0, 1], 'ade': [0.0, 0.0], 'fde': [0.0, 0.0], 'kde': [nan, nan]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(2,20,2)\n",
    "b = 1 * torch.ones(2,12,2)\n",
    "compare_prediction_gt(b,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.2365],\n",
      "         [0.0697],\n",
      "         [0.8606],\n",
      "         [0.2252],\n",
      "         [0.6933],\n",
      "         [0.6357],\n",
      "         [0.9539],\n",
      "         [0.3389],\n",
      "         [0.8169],\n",
      "         [0.0401],\n",
      "         [0.3992],\n",
      "         [0.5462],\n",
      "         [0.8933],\n",
      "         [0.0872],\n",
      "         [0.7167],\n",
      "         [0.3907],\n",
      "         [0.1165],\n",
      "         [0.4998],\n",
      "         [0.9373],\n",
      "         [0.2618]]])\n",
      "tensor([[0.8169]])\n",
      "tensor([[[0.8169]]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.rand(1,20,1)\n",
    "print (t)\n",
    "print (t[:,8,:])\n",
    "print (t[:,8:9,:])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
